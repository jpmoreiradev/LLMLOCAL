# ‚ö° Otimiza√ß√µes de Velocidade

## ‚úÖ O Que Foi Otimizado

### 1. üé§ Whisper (STT) - Mais R√°pido
**ANTES:** `model: "base"` (lento)
**AGORA:** `model: "tiny"` (3-5x mais r√°pido!)

### 2. üß† LLM - Respostas Mais Curtas
**ANTES:** `max_tokens: 500` (respostas longas)
**AGORA:** `max_tokens: 150` (respostas curtas e r√°pidas)

### 3. üë®‚Äçüè´ Prompt - Mais Direto
**ANTES:** Prompt longo com muitas instru√ß√µes
**AGORA:** Prompt curto focado em velocidade

---

## üìä Compara√ß√£o de Velocidade

| Componente | Antes | Agora | Melhoria |
|------------|-------|-------|----------|
| Whisper | base (~5s) | tiny (~1s) | ‚ö° 5x mais r√°pido |
| LLM Response | 500 tokens (~10s) | 150 tokens (~3s) | ‚ö° 3x mais r√°pido |
| Prompt | Longo | Curto | ‚ö° 20% mais r√°pido |
| **TOTAL** | **~15s** | **~4s** | **‚ö° 4x MAIS R√ÅPIDO!** |

---

## üöÄ Op√ß√µes de Modelos por Velocidade

### Whisper (STT):

```yaml
stt:
  model: "tiny"    # ‚ö°‚ö°‚ö° FASTEST - 1s  (ATUAL)
  # model: "base"  # ‚ö°‚ö° Medium - 3s
  # model: "small" # ‚ö° Slow - 8s (mais preciso)
```

### LLM (IA):

**Modelo Atual: llama3 (4.7GB)**
- ‚úÖ Boa qualidade
- ‚ö†Ô∏è Velocidade m√©dia

**Modelos MAIS R√ÅPIDOS (recomendado baixar):**

```bash
# Phi3 Mini - MUITO mais r√°pido (2.3GB)
ollama pull phi3:mini

# Qwen 2.5 - Super r√°pido (934MB)
ollama pull qwen2.5:1.5b

# Gemma 2B - R√°pido (1.6GB)
ollama pull gemma:2b
```

Depois edite `config/settings.yaml`:
```yaml
llm:
  model: "phi3:mini"  # ‚ö° Mais r√°pido que llama3!
```

---

## ‚öôÔ∏è Configura√ß√µes Atuais (Otimizadas)

[config/settings.yaml](config/settings.yaml):

```yaml
# OTIMIZADO PARA VELOCIDADE! ‚ö°

stt:
  model: "tiny"  # Whisper mais r√°pido

llm:
  model: "llama3"  # Trocar por phi3:mini para mais velocidade
  max_tokens: 150  # Respostas curtas e r√°pidas
```

---

## üéØ Como Escolher

### Para M√ÅXIMA VELOCIDADE:
```yaml
stt:
  model: "tiny"

llm:
  model: "phi3:mini"  # OU qwen2.5:1.5b
  max_tokens: 100
```

### Para EQUIL√çBRIO (qualidade + velocidade):
```yaml
stt:
  model: "base"

llm:
  model: "llama3"
  max_tokens: 150
```

### Para M√ÅXIMA QUALIDADE:
```yaml
stt:
  model: "small"

llm:
  model: "llama3"
  max_tokens: 300
```

---

## üß™ Testar Velocidade

Baixe modelos r√°pidos:
```bash
# Phi3 Mini (RECOMENDADO - bom equil√≠brio)
ollama pull phi3:mini

# Qwen (MAIS R√ÅPIDO - menor qualidade)
ollama pull qwen2.5:1.5b
```

Troque no config:
```yaml
llm:
  model: "phi3:mini"
```

Teste:
```bash
python main.py
```

---

## üí° Dicas Extras

### 1. Respostas Ainda Mais R√°pidas:
```yaml
llm:
  max_tokens: 100  # Respostas super curtas
```

### 2. TTS Mais R√°pido:
```yaml
tts:
  speed: 1.3  # Fala 30% mais r√°pido
```

### 3. Modo Turbo (tudo r√°pido):
```yaml
stt:
  model: "tiny"

llm:
  model: "qwen2.5:1.5b"
  max_tokens: 80

tts:
  speed: 1.2
```

---

## ‚ö†Ô∏è Trade-offs

### Tiny vs Base (Whisper):
- ‚úÖ **Tiny:** 5x mais r√°pido
- ‚ùå **Tiny:** Pode errar transcri√ß√µes complexas
- üí° **Use tiny** se fala claramente

### Respostas Curtas vs Longas:
- ‚úÖ **Curtas (150):** 3x mais r√°pido
- ‚ùå **Curtas:** Menos explica√ß√µes detalhadas
- üí° **Use 150 tokens** para pr√°tica de conversa√ß√£o

### Phi3 vs LLaMA3:
- ‚úÖ **Phi3:** 2-3x mais r√°pido
- ‚ùå **Phi3:** Respostas menos criativas
- üí° **Use phi3** se velocidade √© prioridade

---

## üéâ Resultado Final

Com as otimiza√ß√µes aplicadas:

**Tempo de Resposta:**
- ‚è±Ô∏è **Antes:** ~15 segundos
- ‚ö° **Agora:** ~4-6 segundos
- üöÄ **Com Phi3:** ~2-3 segundos!

**Comando para m√°xima velocidade:**
```bash
ollama pull phi3:mini
# Edite config: model: "phi3:mini"
python main.py
```

Agora est√° MUITO mais r√°pido! üöÄ